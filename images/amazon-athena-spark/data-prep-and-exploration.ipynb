{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Input/Output bucket variable. Use the S3 output bucket name that was created during setup step ex: athena-spark-datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python_session"
    }
   },
   "outputs": [],
   "source": [
    "NOAA_OUTPUT_BUCKET=\"athena-spark-datastore\"\n",
    "noaa_csv_prefix = f\"s3://{NOAA_OUTPUT_BUCKET}/noaa_data_csv/\"\n",
    "output_prefix = f\"s3://{NOAA_OUTPUT_BUCKET}/noaa_data_parquet/\"\n",
    "\n",
    "\n",
    "# sparkmagic SQL configs - do not modify\n",
    "spark.conf.set('table.name', 'noaa_data_parquet')\n",
    "spark.conf.set('table.location', output_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python_session"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data for from the S3 path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python_session"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(noaa_csv_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python_session"
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()\n",
    "print(df.show(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific columns and exclude missing values\n",
    "subset_df = df \\\n",
    ".select(\"DATE\", \"STATION\", \"WND\") \\\n",
    ".filter(F.split(df.WND, \",\")[3] != '9999')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python_session"
    }
   },
   "outputs": [],
   "source": [
    "subset_df.printSchema()\n",
    "print(subset_df.show(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python_session"
    }
   },
   "outputs": [],
   "source": [
    "# Parse year and wind speed - scaled backed from the raw data\n",
    "wind_date_df = subset_df \\\n",
    ".withColumn(\"wind_speed\", F.split(subset_df.WND, \",\")[3].cast(DoubleType())/10 ) \\\n",
    ".withColumn(\"measurement_year\", F.year(subset_df.DATE))\\\n",
    ".select(\"station\", \"measurement_year\", \"wind_speed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python_session"
    }
   },
   "outputs": [],
   "source": [
    "wind_date_df.printSchema()\n",
    "print(wind_date_df.show(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python_session"
    }
   },
   "outputs": [],
   "source": [
    "# Find yearly min, avg and max wind speed for each location \n",
    "agg_wind_df = wind_date_df \\\n",
    ".groupBy(\"station\",\"measurement_year\") \\\n",
    ".agg(F.min(wind_date_df.wind_speed).alias(\"min_wind_speed\"),\\\n",
    "F.avg(wind_date_df.wind_speed).alias(\"avg_wind_speed\"),\\\n",
    "F.max(wind_date_df.wind_speed).alias(\"max_wind_speed\")\\\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python_session"
    }
   },
   "outputs": [],
   "source": [
    "agg_wind_df.printSchema()\n",
    "print(agg_wind_df.show(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python_session"
    }
   },
   "outputs": [],
   "source": [
    "# Writing the file output to your local S3 bucket\n",
    "current_time = datetime.now().strftime('%Y-%m-%d-%H-%M')\n",
    "\n",
    "agg_wind_df \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .partitionBy(\"station\") \\\n",
    "    .save(output_prefix)\n",
    "\n",
    "print(\"Finished writing NOAA data out to: \", output_prefix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create table in glue data catalog so we can also query data using Athena Query Editor.\n",
    "\n",
    "# NOTE: Remember to load partitions in Athena Query Editor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "create table if not exists default.${table.name}(\n",
    "          measurement_year int,\n",
    "          min_wind_speed double,\n",
    "          avg_wind_speed double,\n",
    "          max_wind_speed double)\n",
    "    partitioned by (station bigint)\n",
    "    ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
    "    location '${table.location}'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Athena PySpark",
   "language": "python",
   "name": "kepler_python_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "Python_Session",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
